{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "https://vgel.me/posts/handmade-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ops from https://github.com/jaymody/picoGPT/blob/main/gpt2.py (MIT license)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# [m, in], [in, out], [out] -> [m, out]\n",
    "def linear(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "# [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
    "def attention(q, k, v, mask):\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "def causal_self_attention(x, c_attn, c_proj):\n",
    "    # qkv projections\n",
    "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "    # split into qkv\n",
    "    q, k, v = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> 3 of [n_seq, n_embd]\n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "    # perform causal self attention\n",
    "    x = attention(q, k, v, causal_mask)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # out projection\n",
    "    x = linear(x, **c_proj)  # [n_seq, n_embd] @ [n_embd, n_embd] = [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "def transformer_block(x, attn):\n",
    "    x = x + causal_self_attention(x, **attn)\n",
    "    # NOTE: removed ffn\n",
    "    return x\n",
    "\n",
    "# [n_seq] -> [n_seq, n_vocab]\n",
    "def gpt(inputs, wte, wpe, blocks):\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # projection to vocab\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CTX = 5\n",
    "N_VOCAB = 2\n",
    "N_EMBED = 8\n",
    "\n",
    "Lg = 1024  # Large\n",
    "\n",
    "MODEL = {\n",
    "    # EMBEDDING USAGE\n",
    "    #  P = Position embeddings (one-hot)\n",
    "    #  T = Token embeddings (one-hot, first is `a`, second is `b`)\n",
    "    #  V = Prediction scratch space\n",
    "    #\n",
    "    #       [P, P, P, P, P, T, T, V]\n",
    "    \"wte\": np.array(\n",
    "        # one-hot token embeddings\n",
    "        [\n",
    "            [0, 0, 0, 0, 0, 1, 0, 0],  # token `a` (id 0)\n",
    "            [0, 0, 0, 0, 0, 0, 1, 0],  # token `b` (id 1)\n",
    "        ]\n",
    "    ),\n",
    "    \"wpe\": np.array(\n",
    "        # one-hot position embeddings\n",
    "        [\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0],  # position 0\n",
    "            [0, 1, 0, 0, 0, 0, 0, 0],  # position 1\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0],  # position 2\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0],  # position 3\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0],  # position 4\n",
    "        ]\n",
    "    ),\n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"attn\": {\n",
    "                \"c_attn\": {  # generates qkv matrix\n",
    "                    \"b\": np.zeros(N_EMBED * 3),\n",
    "                    \"w\": np.array(\n",
    "                        # this is where the magic happens\n",
    "                        # fmt: off\n",
    "                        [\n",
    "                          [Lg, 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                            1., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                          [Lg, Lg, 0., 0., 0., 0., 0., 0.,  # q\n",
    "                            0., 1., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                          [0., Lg, Lg, 0., 0., 0., 0., 0.,  # q\n",
    "                            0., 0., 1., 0., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                          [0., 0., Lg, Lg, 0., 0., 0., 0.,  # q\n",
    "                            0., 0., 0., 1., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                          [0., 0., 0., Lg, Lg, 0., 0., 0.,  # q\n",
    "                            0., 0., 0., 0., 1., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                          [0., 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                            0., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 1.], # v\n",
    "                          [0., 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                            0., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., -1], # v\n",
    "                          [0., 0., 0., 0., 0., 0., 0., 0.,  # q\n",
    "                            0., 0., 0., 0., 0., 0., 0., 0.,  # k\n",
    "                              0., 0., 0., 0., 0., 0., 0., 0.], # v\n",
    "                        ]\n",
    "                        # fmt: on\n",
    "                    ),\n",
    "                },\n",
    "                \"c_proj\": {  # weights to project attn result back to embedding space\n",
    "                    \"b\": [0, 0, 0, 0, 0, Lg, 0, 0],\n",
    "                    \"w\": np.array(\n",
    "                        [\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                            [0, 0, 0, 0, 0, -Lg, Lg, 0],\n",
    "                        ]\n",
    "                    ),\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = [\"a\", \"b\"]\n",
    "def tokenize(s): return [CHARS.index(c) for c in s]\n",
    "def untok(tok): return CHARS[tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(s):\n",
    "    tokens = tokenize(s)[-5:]\n",
    "    logits = gpt(np.array(tokens), **MODEL)\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        pred = np.argmax(probs[i])\n",
    "        print(\n",
    "            f\"{untok(tok)} ({tok}): next={untok(pred)} ({pred}) probs={probs[i]} logits={logits[i]}\"\n",
    "        )\n",
    "\n",
    "    return np.argmax(probs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(s, max_new_tokens=10):\n",
    "    tokens = tokenize(s)\n",
    "    while len(tokens) < len(s) + max_new_tokens:\n",
    "        logits = gpt(np.array(tokens[-5:]), **MODEL)\n",
    "        probs = softmax(logits)\n",
    "        pred = np.argmax(probs[-1])\n",
    "        tokens.append(pred)\n",
    "    return s + \" :: \" + \"\".join(untok(t) for t in tokens[len(s):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[ 2048. -1023.]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "a (0): next=b (1) probs=[0. 1.] logits=[1.000e+00 1.024e+03]\n",
      "b (1): next=a (0) probs=[1. 0.] logits=[1.024e+03 1.000e+00]\n",
      "a (0): next=a (0) probs=[1. 0.] logits=[1025.    0.]\n",
      "ACCURACY: 100.0% (27 / 27)\n"
     ]
    }
   ],
   "source": [
    "test = \"aab\" * 10\n",
    "total, correct = 0, 0\n",
    "for i in range(2, len(test) - 1):\n",
    "    ctx = test[:i]\n",
    "    expected = test[i]\n",
    "    total += 1\n",
    "    if untok(predict(ctx)) == expected:\n",
    "        correct += 1\n",
    "print(f\"ACCURACY: {correct / total * 100}% ({correct} / {total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
