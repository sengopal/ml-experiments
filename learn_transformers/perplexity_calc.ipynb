{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "https://huggingface.co/docs/transformers/perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 662/662 [00:00<00:00, 1.91MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.51M/2.51M [00:00<00:00, 9.83MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 58.2kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 4.48MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.25MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 235kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "model_id = \"sshleifer/tiny-gpt2\" # \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.48k/8.48k [00:00<00:00, 17.1MB/s]\n",
      "Downloading metadata: 100%|██████████| 6.84k/6.84k [00:00<00:00, 21.3MB/s]\n",
      "Downloading readme: 100%|██████████| 9.62k/9.62k [00:00<00:00, 10.2MB/s]\n",
      "Downloading data: 100%|██████████| 4.72M/4.72M [00:01<00:00, 4.29MB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 35782.92 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 58153.04 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 65861.97 examples/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2240/2248 [07:45<00:01,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 128\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, 1024)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=tensor(10.8185), logits=tensor([[[ 1.2894e-02,  1.0899e-02, -3.1657e-02,  ...,  3.7258e-02,\n",
       "          -3.5729e-05, -2.5439e-02],\n",
       "         [ 1.3005e-02,  1.0992e-02, -3.1928e-02,  ...,  3.7578e-02,\n",
       "          -3.6035e-05, -2.5657e-02],\n",
       "         [-8.3406e-03, -7.0500e-03,  2.0477e-02,  ..., -2.4101e-02,\n",
       "           2.3111e-05,  1.6455e-02],\n",
       "         ...,\n",
       "         [-1.2938e-02, -1.0936e-02,  3.1765e-02,  ..., -3.7386e-02,\n",
       "           3.5851e-05,  2.5526e-02],\n",
       "         [-1.3036e-02, -1.1019e-02,  3.2006e-02,  ..., -3.7670e-02,\n",
       "           3.6122e-05,  2.5720e-02],\n",
       "         [-1.2218e-02, -1.0327e-02,  2.9996e-02,  ..., -3.5304e-02,\n",
       "           3.3855e-05,  2.4105e-02]]]), past_key_values=((tensor([[[[ 0.0074],\n",
       "          [ 0.0075],\n",
       "          [-0.0048],\n",
       "          ...,\n",
       "          [-0.0074],\n",
       "          [-0.0075],\n",
       "          [-0.0070]],\n",
       "\n",
       "         [[-0.0064],\n",
       "          [-0.0064],\n",
       "          [ 0.0042],\n",
       "          ...,\n",
       "          [ 0.0064],\n",
       "          [ 0.0065],\n",
       "          [ 0.0060]]]]), tensor([[[[-0.0614],\n",
       "          [-0.0620],\n",
       "          [ 0.0403],\n",
       "          ...,\n",
       "          [ 0.0617],\n",
       "          [ 0.0621],\n",
       "          [ 0.0582]],\n",
       "\n",
       "         [[-0.0041],\n",
       "          [-0.0042],\n",
       "          [ 0.0027],\n",
       "          ...,\n",
       "          [ 0.0042],\n",
       "          [ 0.0042],\n",
       "          [ 0.0039]]]])), (tensor([[[[-0.0038],\n",
       "          [-0.0039],\n",
       "          [ 0.0024],\n",
       "          ...,\n",
       "          [ 0.0039],\n",
       "          [ 0.0039],\n",
       "          [ 0.0036]],\n",
       "\n",
       "         [[-0.0186],\n",
       "          [-0.0188],\n",
       "          [ 0.0116],\n",
       "          ...,\n",
       "          [ 0.0187],\n",
       "          [ 0.0188],\n",
       "          [ 0.0176]]]]), tensor([[[[-0.0194],\n",
       "          [-0.0196],\n",
       "          [ 0.0121],\n",
       "          ...,\n",
       "          [ 0.0195],\n",
       "          [ 0.0197],\n",
       "          [ 0.0183]],\n",
       "\n",
       "         [[ 0.0062],\n",
       "          [ 0.0062],\n",
       "          [-0.0038],\n",
       "          ...,\n",
       "          [-0.0062],\n",
       "          [-0.0062],\n",
       "          [-0.0058]]]]))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,   837,   284,   423,  4983,\n",
       "           355,  4981,   329, 12549, 19478,   837,  1390, 40892,  9659,   837,\n",
       "          1717,  8121,   837,  1717, 13612,   837,   290, 19478, 12903,  2539,\n",
       "           764,   220,   628,   198]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(begin_loc, end_loc, encodings.input_ids[:, begin_loc:end_loc])\n",
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8245)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 287644])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1024,\n",
       " 2048,\n",
       " 3072,\n",
       " 4096,\n",
       " 5120,\n",
       " 6144,\n",
       " 7168,\n",
       " 8192,\n",
       " 9216,\n",
       " 10240,\n",
       " 11264,\n",
       " 12288,\n",
       " 13312,\n",
       " 14336,\n",
       " 15360,\n",
       " 16384,\n",
       " 17408,\n",
       " 18432,\n",
       " 19456,\n",
       " 20480,\n",
       " 21504,\n",
       " 22528,\n",
       " 23552,\n",
       " 24576,\n",
       " 25600,\n",
       " 26624,\n",
       " 27648,\n",
       " 28672,\n",
       " 29696,\n",
       " 30720,\n",
       " 31744,\n",
       " 32768,\n",
       " 33792,\n",
       " 34816,\n",
       " 35840,\n",
       " 36864,\n",
       " 37888,\n",
       " 38912,\n",
       " 39936,\n",
       " 40960,\n",
       " 41984,\n",
       " 43008,\n",
       " 44032,\n",
       " 45056,\n",
       " 46080,\n",
       " 47104,\n",
       " 48128,\n",
       " 49152,\n",
       " 50176,\n",
       " 51200,\n",
       " 52224,\n",
       " 53248,\n",
       " 54272,\n",
       " 55296,\n",
       " 56320,\n",
       " 57344,\n",
       " 58368,\n",
       " 59392,\n",
       " 60416,\n",
       " 61440,\n",
       " 62464,\n",
       " 63488,\n",
       " 64512,\n",
       " 65536,\n",
       " 66560,\n",
       " 67584,\n",
       " 68608,\n",
       " 69632,\n",
       " 70656,\n",
       " 71680,\n",
       " 72704,\n",
       " 73728,\n",
       " 74752,\n",
       " 75776,\n",
       " 76800,\n",
       " 77824,\n",
       " 78848,\n",
       " 79872,\n",
       " 80896,\n",
       " 81920,\n",
       " 82944,\n",
       " 83968,\n",
       " 84992,\n",
       " 86016,\n",
       " 87040,\n",
       " 88064,\n",
       " 89088,\n",
       " 90112,\n",
       " 91136,\n",
       " 92160,\n",
       " 93184,\n",
       " 94208,\n",
       " 95232,\n",
       " 96256,\n",
       " 97280,\n",
       " 98304,\n",
       " 99328,\n",
       " 100352,\n",
       " 101376,\n",
       " 102400,\n",
       " 103424,\n",
       " 104448,\n",
       " 105472,\n",
       " 106496,\n",
       " 107520,\n",
       " 108544,\n",
       " 109568,\n",
       " 110592,\n",
       " 111616,\n",
       " 112640,\n",
       " 113664,\n",
       " 114688,\n",
       " 115712,\n",
       " 116736,\n",
       " 117760,\n",
       " 118784,\n",
       " 119808,\n",
       " 120832,\n",
       " 121856,\n",
       " 122880,\n",
       " 123904,\n",
       " 124928,\n",
       " 125952,\n",
       " 126976,\n",
       " 128000,\n",
       " 129024,\n",
       " 130048,\n",
       " 131072,\n",
       " 132096,\n",
       " 133120,\n",
       " 134144,\n",
       " 135168,\n",
       " 136192,\n",
       " 137216,\n",
       " 138240,\n",
       " 139264,\n",
       " 140288,\n",
       " 141312,\n",
       " 142336,\n",
       " 143360,\n",
       " 144384,\n",
       " 145408,\n",
       " 146432,\n",
       " 147456,\n",
       " 148480,\n",
       " 149504,\n",
       " 150528,\n",
       " 151552,\n",
       " 152576,\n",
       " 153600,\n",
       " 154624,\n",
       " 155648,\n",
       " 156672,\n",
       " 157696,\n",
       " 158720,\n",
       " 159744,\n",
       " 160768,\n",
       " 161792,\n",
       " 162816,\n",
       " 163840,\n",
       " 164864,\n",
       " 165888,\n",
       " 166912,\n",
       " 167936,\n",
       " 168960,\n",
       " 169984,\n",
       " 171008,\n",
       " 172032,\n",
       " 173056,\n",
       " 174080,\n",
       " 175104,\n",
       " 176128,\n",
       " 177152,\n",
       " 178176,\n",
       " 179200,\n",
       " 180224,\n",
       " 181248,\n",
       " 182272,\n",
       " 183296,\n",
       " 184320,\n",
       " 185344,\n",
       " 186368,\n",
       " 187392,\n",
       " 188416,\n",
       " 189440,\n",
       " 190464,\n",
       " 191488,\n",
       " 192512,\n",
       " 193536,\n",
       " 194560,\n",
       " 195584,\n",
       " 196608,\n",
       " 197632,\n",
       " 198656,\n",
       " 199680,\n",
       " 200704,\n",
       " 201728,\n",
       " 202752,\n",
       " 203776,\n",
       " 204800,\n",
       " 205824,\n",
       " 206848,\n",
       " 207872,\n",
       " 208896,\n",
       " 209920,\n",
       " 210944,\n",
       " 211968,\n",
       " 212992,\n",
       " 214016,\n",
       " 215040,\n",
       " 216064,\n",
       " 217088,\n",
       " 218112,\n",
       " 219136,\n",
       " 220160,\n",
       " 221184,\n",
       " 222208,\n",
       " 223232,\n",
       " 224256,\n",
       " 225280,\n",
       " 226304,\n",
       " 227328,\n",
       " 228352,\n",
       " 229376,\n",
       " 230400,\n",
       " 231424,\n",
       " 232448,\n",
       " 233472,\n",
       " 234496,\n",
       " 235520,\n",
       " 236544,\n",
       " 237568,\n",
       " 238592,\n",
       " 239616,\n",
       " 240640,\n",
       " 241664,\n",
       " 242688,\n",
       " 243712,\n",
       " 244736,\n",
       " 245760,\n",
       " 246784,\n",
       " 247808,\n",
       " 248832,\n",
       " 249856,\n",
       " 250880,\n",
       " 251904,\n",
       " 252928,\n",
       " 253952,\n",
       " 254976,\n",
       " 256000,\n",
       " 257024,\n",
       " 258048,\n",
       " 259072,\n",
       " 260096,\n",
       " 261120,\n",
       " 262144,\n",
       " 263168,\n",
       " 264192,\n",
       " 265216,\n",
       " 266240,\n",
       " 267264,\n",
       " 268288,\n",
       " 269312,\n",
       " 270336,\n",
       " 271360,\n",
       " 272384,\n",
       " 273408,\n",
       " 274432,\n",
       " 275456,\n",
       " 276480,\n",
       " 277504,\n",
       " 278528,\n",
       " 279552,\n",
       " 280576,\n",
       " 281600,\n",
       " 282624,\n",
       " 283648,\n",
       " 284672,\n",
       " 285696,\n",
       " 286720]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, seq_len, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
