{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/vllm-project/vllm/issues/367\n",
    "# Just dropping this as a simple example of how to do data parallel inference in Python that I've found to be effective.\n",
    "\n",
    "# Obviously I'd appreciate the eventual implementation of proper data parallel processing in the actual package, but this works decently as a stop-gap just now.\n",
    "\n",
    "import os\n",
    "import multiprocessing\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "NUM_GPUS = 4\n",
    "\n",
    "def run_inference_one_gpu(gpu_id, prompt_list, model_name, sampling_params):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    llm = LLM(model=model_name)\n",
    "    return llm.generate(prompt_list, sampling_params)\n",
    "\n",
    "# Splits a list into roughly equally sized pieces\n",
    "# split_list([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"], 3) -> [['a', 'b'], ['c', 'd'], ['e', 'f', 'g']]\n",
    "split_list = lambda l, n: [l[i * len(l) // n: (i + 1) * len(l) // n] for i in range(n)]\n",
    "\n",
    "def run_inference_multi_gpu(model_name, prompts, sampling_params):\n",
    "    split_prompts = split_list(prompts, NUM_GPUS)\n",
    "    inputs = [(i, p, model_name, sampling_params) for i, p in enumerate(split_prompts)]\n",
    "\n",
    "    with multiprocessing.Pool(processes=NUM_GPUS) as pool:\n",
    "        results = pool.starmap(run_inference_one_gpu, inputs)\n",
    "\n",
    "    outputs = []\n",
    "    for result in results:\n",
    "        outputs.extend(result)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "prompts = [f\"Write me a story about why {i} is your favourite number.\\n\\n{i} is my favourite number because \" for i in range(100)]\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=256)\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "outputs = run_inference_multi_gpu(model_name, prompts, sampling_params)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
